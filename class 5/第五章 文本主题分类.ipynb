{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第五章 文本主题分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP简介\n",
    "### 1. NLP定义\n",
    "自然语言处理NLP (Natural Language Processing) 是人工智能（AI）的一个子领域，指让机器理解并解释人类写作、说话方式的能力。NLP 的目标是让机器在理解语言上像人类一样智能。最终目标是弥补人类交流（自然语言）和计算机理解（机器语言）之间的差距。\n",
    "### 2. NLP建模流程\n",
    "获取语料 -> 预料预处理 -> 特征工程 -> 特征选择 -> 模型训练 -> 交叉验证 -> 部署上线"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取预料\n",
    "- 已有语料，公司随业务发展积累的文本资料，如聊天记录，客户评论。。\n",
    "- 国内外标准开放数据，如搜狗语料、人民日报语料，或者爬虫抓取  \n",
    "\n",
    "与之前数据挖掘不同的地方是，之前的数据集都是结构化的数据，有明确定义的数据类型，人的年龄信息、性别信息，所有样本的数据结构都是相同的；而文本属于非结构化的数据，没有明确的内部结构，因此数据的处理方式也有所不同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预料预处理\n",
    "#### 1. 语料清洗\n",
    "数据清洗，顾名思义就是在语料中找到我们感兴趣的东西，把不感兴趣的、视为噪音的内容清洗删除，包括对于原始文本提取标题、摘要、正文等信息，对于爬取的网页内容，去除广告、标签、HTML、JS 等代码和注释等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 分词\n",
    "中文语料数据为一批短文本或者长文本，比如：句子，文章摘要，段落或者整篇文章组成的一个集合。一般句子、段落之间的字、词语是连续的，有一定含义。而进行文本挖掘分析时，我们希望文本处理的最小单位粒度是词或者词语，所以这个时候就需要分词来将文本全部进行分词。\n",
    "\n",
    "当前中文分词算法的主要难点有歧义识别和新词识别，比如：“羽毛球拍卖完了”，这个可以切分成“羽毛 球拍 卖 完 了”，也可切分成“羽毛球 拍卖 完 了”，如果不依赖上下文其他的句子，恐怕很难知道如何去理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 词性标注\n",
    "词性标注，就是给每个词或者词语打词类标签，如形容词、动词、名词等。这样做可以让文本在后面的处理中融入更多有用的语言信息。词性标注是一个经典的序列标注问题，不过对于有些中文自然语言处理来说，词性标注不是非必需的。比如，常见的文本分类就不用关心词性问题，但是类似情感分析、知识推理却是需要的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 去停用词\n",
    "停用词一般指对文本特征没有任何贡献作用的字词，比如标点符号、语气、人称等一些词。所以在一般性的文本处理中，分词之后，接下来一步就是去停用词。但是对于中文来说，去停用词操作不是一成不变的，停用词词典是根据具体场景来决定的，比如在情感分析中，语气词、感叹号是应该保留的，因为他们对表示语气程度、感情色彩有一定的贡献和意义。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征工程\n",
    "做完语料预处理之后，接下来需要考虑如何把分词之后的字和词语表示成计算机能够计算的类型。显然，如果要计算我们至少需要把中文分词的字符串转换成数字，确切的说应该是数学中的向量。有两种常用的表示模型分别是词袋模型和词向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 词袋模型\n",
    "词袋模型（Bag of Word, BOW)，即不考虑词语原本在句子中的顺序，直接将每一个词语或者符号统一放置在一个集合（如 list），然后按照计数的方式对出现的次数进行统计。统计词频这只是最基本的方式，除此之外还有one-hot编码，TF-IDF 等。词袋模型不考虑词语出现的顺序，将每个出现的词汇单独作为一个特征，也就是说，词袋模型不会考虑上下文的信息，比如对于“程序员”和“编程”，他只会统计每个词出现的信息，却学习不到两个词之间的关系。\n",
    "#### one-hot编码\n",
    "<img src=\"./material/one-hot 编码1.webp\" width=\"700px\" height=\"700px\"/>\n",
    "<img src=\"./material/one-hot 编码2.webp\" width=\"700px\" height=\"700px\"/>\n",
    "\n",
    "- 优点：一是解决了分类器不好处理离散数据的问题，二是在一定程度上也起到了扩充特征的作用。\n",
    "- 缺点：在文本特征表示上有些缺点就非常突出了。首先，它是一个词袋模型，不考虑词与词之间的顺序（文本中词的顺序信息也是很重要的）；其次，它假设词与词相互独立（在大多数情况下，词与词是相互影响的）；最后，它得到的特征是离散稀疏的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 词向量\n",
    "词向量是将字、词语转换成向量矩阵的计算模型，他同时会考虑词语出现顺序的情况，可以获取到文章上下文的信息，这是词向量相比与词袋模型天然的优势。例如Google的Word2Vec等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征选择\n",
    "同数据挖掘一样，在文本挖掘相关问题中，特征工程也是必不可少的。在一个实际问题中，构造好的特征向量，大多时候是很稀疏的，是要选择合适的、表达能力强的特征。文本特征一般都是词语，具有语义信息，我们要找出一个特征子集，其可以尽可能保留语义信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关键词提取：TF-IDF\n",
    "### 1. 余弦相似度\n",
    "#### 相似度\n",
    "一个向量空间中两个向量夹角的余弦值作为衡量两个个体之间差异的大小。\n",
    "- 余弦值接近1，夹角趋于0，表明两个向量越相似。\n",
    "![](./material/余弦计算图.webp)  \n",
    "<img src=\"./material/余弦相似度计算公式.webp\" width=\"700px\" height=\"700px\"/>\n",
    "\n",
    "#### 例子\n",
    "<img src=\"./material/余弦相似度栗子.webp\" width=\"700px\" height=\"700px\"/>\n",
    "\n",
    "#### 计算流程\n",
    "得到了文本相似度计算的处理流程是:\n",
    "- 找出两篇文章的关键词；\n",
    "- 每篇文章各取出若干个关键词，合并成一个集合，计算每篇文章对于这个集合中的词的词频\n",
    "- 生成两篇文章各自的词频向量；\n",
    "- 计算两个向量的余弦相似度，值越大就表示越相似。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. TF-IDF原理 \n",
    "#### 词频TF\n",
    "词频——TF（Term Frequency）：一个词在文章中出现的次数。假设：如果一个词很重要，应该会在文章中多次出现，但这也不是绝对的，出现次数最多的助词“的”“是”“在”，这类最常用的词，其实对我们分析文本没用任何作用，叫做停用词（stop words），停用词对结果毫无帮助，必须过滤掉的词。\n",
    "\n",
    "过滤掉停用词后就一定能接近问题么？进一步调整假设：如果某个词在其他文本中比较少见，但是它在这篇文章中多次出现，那么它很可能反映了这篇文章的特性，正是我们所需要的关键词。\n",
    "\n",
    "#### 反文档频率IDF\n",
    "在词频的基础上，赋予每一个词的权重，进一步体现该词的重要性，权重越大代表该词越重要。最常见的词（“的”、“是”、“在”）给予最小的权重,较常见的词（“国内”、“中国”、“报道”）给予较小的权重,较少见的词（“养殖”、“维基”）给予较大的权重。\n",
    "\n",
    "• 将TF和IDF进行相乘，就得到了一个词的TF-IDF值，某个词对文章重要性越高，该值越大，于是排在前面的几个词，就是这篇文章的关键词。\n",
    "\n",
    "#### 计算步骤\n",
    "<img src=\"./material/TF-IDF计算步骤.webp\" width=\"700px\" height=\"700px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 关键词提取\n",
    "#### 基于 TF-IDF 算法的关键词抽取\n",
    "import jieba.analyse 计算并返回一段文本tf-idf最大的一组词\n",
    "- jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())\n",
    "> - sentence 为待提取的文本  \n",
    "> - topK 为返回几个 TF/IDF 权重最大的关键词，默认值为 20  \n",
    "> - withWeight 为是否一并返回关键词权重值，默认值为 False  \n",
    "> - allowPOS 仅包括指定词性的词，默认值为空，即不筛选  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                            content\n",
      "0           0  　　,中新网,1月7日电  恰逢CES 2017拉开大幕，却惊闻“AlphaGo升级版”的M...\n",
      "1           1  　　徐立，商汤科技CEO在谈起本次参展时谈到：“作为一个成立刚刚两年的创业公司，这次参展，一...\n",
      "2           2  　　正如最后挑战Master的古力在落败后发表的观点：“人类与人工智能共同探索围棋世界的大幕...\n",
      "3           3                             　,　SenseFace人脸布控的“黑科技”\n",
      "4           4  　　本届大展最大的看点，无疑是“被誉为2016全美科技第一神股”英伟达的首次CES主题演讲。...\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse as analyse\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./data/technology_news.csv\", encoding='utf-8')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户  2016  互联网  手机  平台  人工智能  百度  2017  智能  技术  数据  360  服务  直播  产品  企业  安全  视频  移动  应用  网络  行业  游戏  机器人  电商  内容  中国  领域  通过  发展\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "lines=df.content.values.tolist()\n",
    "content = \"\".join(lines) \n",
    "print(\"  \".join(analyse.extract_tags(content, topK=30, withWeight=False, allowPOS=())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "航母  训练  海军  中国  官兵  部队  编队  作战  10  任务  美国  导弹  能力  20  2016  军事  无人机  装备  进行  记者  我们  军队  安全  保障  12  战略  军人  日本  南海  战机\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse as analyse\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./data/military_news.csv\", encoding='utf-8')\n",
    "df = df.dropna()  # 去掉为空的样本\n",
    "lines=df.content.values.tolist()\n",
    "content = \"\".join(lines)  # 将一个列表中元素以特定的间隔合并成一个字符串\n",
    "print(\"  \".join(analyse.extract_tags(content, topK=30, withWeight=False, allowPOS=())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本主题分类模型\n",
    "### 1. 朴素贝叶斯分类器\n",
    "朴素贝叶斯基于一下两点：\n",
    "- 假设各特征之间相互独立；\n",
    "- 贝叶斯公式：$$P(B|A) = \\frac{P(A|B)*P(B)}{P(A)}$$\n",
    "\n",
    "#### 例子\n",
    "<img src=\"./material/朴素贝叶斯栗子.png\" width=\"700px\" height=\"700px\"/>\n",
    "\n",
    "现在给我们的问题是，如果一对男女朋友，男生想女生求婚，男生的四个特点分别是不帅，性格不好，身高矮，不上进，请你判断一下女生是嫁还是不嫁？\n",
    "\n",
    "这是典型的二分类问题，按照朴素贝叶斯的求解，转换为P(嫁|不帅、性格不好、矮、不上进)和P(不嫁|不帅、性格不好、矮、不上进)的概率，最终选择嫁与不嫁的答案。\n",
    "<img src=\"./material/贝叶斯求婚.png\" width=\"700px\" height=\"700px\"/>\n",
    "\n",
    "P(嫁)=1/2、P(不帅|嫁)=1/2、P(性格不好|嫁)=1/6、P(矮|嫁)=1/6、P(不上进|嫁)=1/6。\n",
    "P(不嫁)=1/2、P(不帅|不嫁)=1/3、P(性格不好|不嫁)=1/2、P(矮|不嫁)=1、P(不上进|不嫁)=2/3\n",
    "\n",
    "**对于目标求解为不同的类别，贝叶斯公式的分母总是相同的。**\n",
    "\n",
    "于是，对于类别“嫁”的贝叶斯分子为：P(嫁)P(不帅|嫁)P(性格不好|嫁)P(矮|嫁)P(不上进|嫁)=1/2 * 1/2 * 1/6 * 1/6 * 1/6=1/864     \n",
    "对于类别“不嫁”的贝叶斯分子为:P(不嫁)P(不帅|不嫁)P(性格不好|不嫁)P(矮|不嫁)P(不上进|不嫁)=1/2 * 1/3 * 1/2 * 1* 2/3=1/18。\n",
    "经代入贝叶斯公式可得：P(嫁|不帅、性格不好、矮、不上进)=(1/864) / (1/864+1/18)=1/49=2.04%\n",
    "\n",
    "**最终**P(不嫁|不帅、性格不好、矮、不上进)=(1/18) / (1/864+1/18)=48/49=97.96%\n",
    "\n",
    "则P(不嫁|不帅、性格不好、矮、不上进) > P(嫁|不帅、性格不好、矮、不上进)，则该女子选择不嫁！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般在数据量足够，数据丰富度够的情况下，用朴素贝叶斯完成这个任务，准确度还是很不错的。因为朴素贝叶斯认为所有的特征之间都是独立的，而我们的词袋模型得到的特征，因为没有考虑词与词之间的关系，也可以认为是相互独立的。朴素贝叶斯计算公式简单，因此速度非常快，因此它也常用与简单的文本分类问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 文本主题分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 准备数据\n",
    "准备好数据，我们挑选 科技、汽车、娱乐、军事、运动 总共5类文本数据进行处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "df_technology = pd.read_csv(\"./data/technology_news.csv\", encoding='utf-8')\n",
    "df_technology = df_technology.dropna()\n",
    "\n",
    "df_car = pd.read_csv(\"./data/car_news.csv\", encoding='utf-8')\n",
    "df_car = df_car.dropna()\n",
    "\n",
    "df_entertainment = pd.read_csv(\"./data/entertainment_news.csv\", encoding='utf-8')\n",
    "df_entertainment = df_entertainment.dropna()\n",
    "\n",
    "df_military = pd.read_csv(\"./data/military_news.csv\", encoding='utf-8')\n",
    "df_military = df_military.dropna()\n",
    "\n",
    "df_sports = pd.read_csv(\"./data/sports_news.csv\", encoding='utf-8')\n",
    "df_sports = df_sports.dropna()\n",
    "\n",
    "technology = df_technology.content.values.tolist()[1000:2100]\n",
    "car = df_car.content.values.tolist()[1000:2100]\n",
    "entertainment = df_entertainment.content.values.tolist()[:2000]\n",
    "military = df_military.content.values.tolist()[:2000]\n",
    "sports = df_sports.content.values.tolist()[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随便挑几条看下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "　　现在家里都拉了网线，都能无线上网，一定要帮他们先登上WiFi，另外，老人不懂得流量是什么，也不知道如何开关，控制流量，所以设置好流量上限很重要，免得不小心点开了视频或者下载，电话费就大发了。\n"
     ]
    }
   ],
   "source": [
    "print(technology[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "　　截至发稿时，人人车给出的处理方案仍旧是检修车辆。王先生则认为，车辆在购买时就存在问题，但交易平台并未能检测出来。因此，王先生希望对方退款。王先生称，他将找专业机构对车辆进行鉴定，并通过法律途径维护自己的权益。J256\n"
     ]
    }
   ],
   "source": [
    "print(car[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "　　网综尺度相对较大原本是制作优势，《奇葩说》也曾经因为讨论的话题较为前卫一度引发争议。但《奇葩说》对于价值观的把握和引导让其中内含的“少儿不宜”只能算是小花絮。而纯粹是为了制造话题而“污”得“无节操无下限”的网综不仅让人生厌，也给节目自身招致了下架的厄运。对资本方而言，即便只从商业运营考量，点击量也分有价值和无价值，节目内容的变现能力如果建立在博眼球和低趣味迎合上，商业运营也难长久。对节目制作方与平台来说，为博一时的高点击而不顾底线不仅是砸自己招牌，以噱头吸引而来的观众与流量也是难以维持。\n"
     ]
    }
   ],
   "source": [
    "print(entertainment[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分词与中文文本处理\n",
    "##### 去停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=pd.read_csv(\"data/stopwords.txt\",index_col=False,quoting=3,sep=\"\\t\",names=['stopword'], encoding='utf-8')\n",
    "stopwords=stopwords['stopword'].values\n",
    "\n",
    "\n",
    "def preprocess_text(content_lines, sentences, category):\n",
    "    for line in content_lines:\n",
    "        try:\n",
    "            segs = jieba.lcut(line)  # jieba.lcut对句子做分词\n",
    "#             print(segs)\n",
    "            segs = filter(lambda x:len(x)>1, segs)  # 去掉为空的句子\n",
    "            segs = filter(lambda x:x not in stopwords, segs)  # 去掉停用词\n",
    "            sentences.append((\" \".join(segs), category))  # 将特征与标签以\" \"连起来\n",
    "        except Exception:\n",
    "            print(line)\n",
    "            continue \n",
    "\n",
    "#生成训练数据\n",
    "sentences = []\n",
    "\n",
    "preprocess_text(technology, sentences, 'technology')\n",
    "preprocess_text(car, sentences, 'car')\n",
    "preprocess_text(entertainment, sentences, 'entertainment')\n",
    "preprocess_text(military, sentences, 'military')\n",
    "preprocess_text(sports, sentences, 'sports')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成训练集与测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(sentences)  # 打乱顺序，生成更可靠的训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "北京 联合 张家口 申办 2022 冬奥会 成功 筹备 阶段 奥组委 制定 总体规划 场馆 建设 交通设施 逐一 落实 赛事 基础 规划 交付 计划 陆续 提交 国际奥委会 sports\n",
      "女子监狱 大火 罗丝 极限 特工 终极 回归 饰演 狙击手 阿黛尔 片中 一头 绿色 短发 犀利 眼神 敏捷 动作 精准 枪法 个性 十足 花臂 置身于 硬汉 特工 群体 毫不逊色 特辑 阿黛尔 蛰伏 草原 干净利落 击中 偷猎者 以此 保护 野生 雄狮 担当 草原 捍卫者 桑德 凯奇 得力助手 阿黛尔堪 神枪手 相隔 一栋 大楼 从桑德 凯奇 手指 间隙 中将 敌人 击毙 霸气 十足 entertainment\n",
      "护航 编队 起航 之初 海况 恶劣 女舰员 晕船 宋玺 晕船 很大 呕吐 海盗 赶走 倒下 晕船 这一关 宋玺 豁出去 风浪 多大 训练 运动 强度 逐天 加大 每次 东西 时间 一天天 身体 大海 同频 共振 military\n",
      "足代会 参会 人员 规模 庞大 包含 中国足协 顾问 主席 主席 执委 中国足协 会员 协会 代表 体育总局 相关 部门 中国足协 专项 委员会 负责人 中国足协 秘书长 部门 负责人 中超 中甲 女超 女甲 中超 公司 中国 足球 部门 2026 世界杯 队伍 足代会 讨论 重点 聚焦 sports\n",
      "学习 对手 研究 对手 战胜 对手 空军 新锐 一代 飞行员 矢志 强军 制胜 空天 座右铭 提升 实战 能力 牵引 空军 实战 训练 研究 队友 研究 对手 研究 考场 研究 战场 研究 考核 规则 规则 战场 作战 环境 2017 空军 将会 实战 训练 三个 坚定 向前 推进 改革 强军 壮志 将会 锐不可当 military\n",
      "最早 蒸汽 弹射 英国海军 航空兵 后备 司令 米切尔 1951 率先 研制成功 装备 莫仙座 航母 美国 海军 购买 这项 专利 法等国 航母 标准配置 俄罗斯 航母 研发 滞后 1982 蒸汽 弹射器 研究 计划 年代 服役 乌里扬诺夫斯克 核动力 航母 安装 蒸汽 弹射器 可惜 一款 航母 苏联 解体 夭折 military\n",
      "斯诺克 大师赛 丁俊晖 福地 2011 赢得 冠军 击败 中国香港 名将 傅家俊 大师赛 斯诺克 排名赛 分量 上同 世锦赛 英锦赛 列为 英国 传统 大赛 2012 连续 大师赛 首轮 止步 负于 奥沙利 罗伯逊 宾汉姆 墨菲 佩里 八强 昨天 对手 威尔逊 斯诺克 新星 拿到 排名赛 冠军 亚军 夺冠 道路 击败 丁俊晖 昨天 比赛 丁俊晖 表现 稳健 始终 掌控 局面 最终 获胜 第二局 比赛 丁俊晖 本来 机会 打出 满分 可惜 黄球 失手 sports\n",
      "伤残 军人 买不到 半价票 military\n",
      "本报讯 李远飞 昨天 纳达尔 苦战 挺进 澳网 男单 赛后 给予 对手 亚历山大 维列夫 评价 网球 未来 希望 徐一 搭档 梁辰 杨钊煊 昨天 击败 对手 澳网 女双 八强 相遇 sports\n",
      "韩寒 澄清 两首歌 自始至终 描写 自卑 笨拙 努力 男人 生计 奔波 一生 女人 韩寒 希望 理性 发言 上纲上线 entertainment\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences[:10]:\n",
    "    print(sentence[0], sentence[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6150"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x, y = zip(*sentences)  # 将特征与标签分割开\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1234)\n",
    "len(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下一步要做的就是在降噪数据上抽取出来有用的特征啦，我们对文本抽取词袋模型特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=4000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# 将语料抽取为词袋模型特征\n",
    "vec = CountVectorizer(\n",
    "    analyzer='word', # tokenise by character ngrams\n",
    "    max_features=4000,  # keep the most common 4000 ngrams\n",
    ")\n",
    "vec.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vec.vocabulary_)\n",
    "\n",
    "def get_features(x):\n",
    "    vec.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 436)\t1\n",
      "  (0, 1544)\t1\n",
      "  (0, 1938)\t1\n",
      "  (1, 232)\t1\n",
      "  (1, 1329)\t1\n"
     ]
    }
   ],
   "source": [
    "string1 = \"小米 9 的 小爱同学 使用 了 人工智能 技术\"\n",
    "string2 = \"今天 天气 真 不错 我们 出去 打 球吧\"\n",
    "print(vec.transform([string1, string2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用朴素贝叶斯进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB  # 导入朴素贝叶斯分类器\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(vec.transform(x_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8907317073170732"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(vec.transform(x_test), y_test)  # 看下准确率如何"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['military']\n"
     ]
    }
   ],
   "source": [
    "print(classifier.predict(vec.transform(['这 是 有史以来 最 大 的 一 次 军舰 演习'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['technology']\n"
     ]
    }
   ],
   "source": [
    "print(classifier.predict(vec.transform(['小米 9 的 小爱同学 使用 了 人工智能 技术'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他NLP相关算法\n",
    "### Google：Word2Vec\n",
    "2013年，Google开源了一款用于词向量计算的工具——word2vec，引起了工业界和学术界的关注。首先，word2vec可以在百万数量级的词典和上亿的数据集上进行高效地训练；其次，该工具得到的训练结果——词向量（word embedding），可以很好地度量词与词之间联系。\n",
    "### FaceBook：FastText \n",
    "fastText是一种Facebook AI Research在16年开源的一个文本分类器。 其特点就是fast。相对于其它文本分类模型，如SVM，Logistic Regression和neural network等模型，fastText在保持分类效果的同时，大大缩短了训练时间。\n",
    "\n",
    "适合大型数据+高效的训练速度，FastText的性能要比时下流行的word2vec工具明显好上不少。\n",
    "### 其他常用深度学习模型：LSTM, CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结&课堂任务\n",
    "### 总结\n",
    "<img src=\"./material/第五章总结.png\" width=\"500px\" height=\"500px\"/>\n",
    "\n",
    "### 课堂任务\n",
    "- 复习本节课内容\n",
    "- 自己用其他分类器跑一下文本分类的模型\n",
    "- 自学word2vec和fasttext等其他nlp相关算法模型"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
